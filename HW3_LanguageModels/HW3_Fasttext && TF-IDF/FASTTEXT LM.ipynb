{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da4ce0f",
   "metadata": {},
   "source": [
    "# جهت اجرای نوت‌بوک لطفا فایل زیر را دانلود کنید و در فولدر مربوط به همین فایل قرار دهید.\n",
    "\n",
    "\n",
    "https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.fa.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2b185",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bbf2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "import fasttext\n",
    "from hazm import *\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tqdm\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ed456",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4222e553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    2975\n",
       " 1    2354\n",
       " 0    1879\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pre-proc-data2.csv')\n",
    "text = df['comment']\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c8b25",
   "metadata": {},
   "source": [
    "### Preparing the dataset to be compatible with the fasttext structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379b0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(-1 , 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b36eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>آقای رشیدپور من تا هفته NUM۲ بچه‌ام پسر بود و ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>چقدر این دزد باحال بود</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>سلام اگر ممکنه از کلمه سی تی اسکن تو برنامتون ...</td>\n",
       "      <td>__label__neg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            comment  \\\n",
       "0           0  آقای رشیدپور من تا هفته NUM۲ بچه‌ام پسر بود و ...   \n",
       "1           1                            چقدر این دزد باحال بود    \n",
       "2           2  سلام اگر ممکنه از کلمه سی تی اسکن تو برنامتون ...   \n",
       "\n",
       "      sentiment  num_tok  \n",
       "0    __label__1       31  \n",
       "1    __label__1        5  \n",
       "2  __label__neg       19  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = \"__label__\" + df['sentiment'].astype(str)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a37f019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_tok</th>\n",
       "      <th>sentiment_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>آقای رشیدپور من تا هفته NUM۲ بچه‌ام پسر بود و ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>31</td>\n",
       "      <td>__label__1 آقای رشیدپور من تا هفته NUM۲ بچه‌ام...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>چقدر این دزد باحال بود</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>5</td>\n",
       "      <td>__label__1 چقدر این دزد باحال بود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>سلام اگر ممکنه از کلمه سی تی اسکن تو برنامتون ...</td>\n",
       "      <td>__label__neg</td>\n",
       "      <td>19</td>\n",
       "      <td>__label__neg سلام اگر ممکنه از کلمه سی تی اسکن...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            comment  \\\n",
       "0           0  آقای رشیدپور من تا هفته NUM۲ بچه‌ام پسر بود و ...   \n",
       "1           1                            چقدر این دزد باحال بود    \n",
       "2           2  سلام اگر ممکنه از کلمه سی تی اسکن تو برنامتون ...   \n",
       "\n",
       "      sentiment  num_tok                                  sentiment_comment  \n",
       "0    __label__1       31  __label__1 آقای رشیدپور من تا هفته NUM۲ بچه‌ام...  \n",
       "1    __label__1        5                 __label__1 چقدر این دزد باحال بود   \n",
       "2  __label__neg       19  __label__neg سلام اگر ممکنه از کلمه سی تی اسکن...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_comment'] = df['sentiment'] +' '+ df['comment']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23972d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(text):\n",
    "    text = re.sub(r'[^\\w\\s]',' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e52bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_comment'] = [pre(str(i)) for i in df['sentiment_comment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb350e81",
   "metadata": {},
   "source": [
    "### split dataset to train, test and valid sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8493c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.1, shuffle=True)\n",
    "train , valid = train_test_split(train, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b272c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"t.train\", columns= ['sentiment_comment'], index = False, header = False)\n",
    "test.to_csv(\"t.test\", columns= ['sentiment_comment'], index = False, header = False)\n",
    "valid.to_csv(\"t.valid\", columns= ['sentiment_comment'], index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803edf97",
   "metadata": {},
   "source": [
    "### train a model using fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "054a5715",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:   11 Best score:  0.765794 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  14904\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:   80944 lr:  0.000000 avg.loss:  0.500434 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(\"t.train\", dim=300, autotuneValidationFile='t.valid', pretrainedVectors='wiki.fa.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39aa563a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 0.7323162274618585, 0.7323162274618585)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('t.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183eef29",
   "metadata": {},
   "source": [
    "### Accuracy without Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f06a68d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 87.1012482662968%\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(np.array(test).shape[0]):\n",
    "    if model.predict(np.array(test['comment'])[i])[0][0] == np.array(test['sentiment'])[i] : \n",
    "        acc += 1\n",
    "print(f'accuracy is {acc/np.array(test).shape[0] *100}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7584abd",
   "metadata": {},
   "source": [
    "### -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7d38f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "stemmer = Stemmer()\n",
    "tagger = POSTagger(model='resources-0/postagger.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c95cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "def same_root(word1, word2, n=3):\n",
    "    # Extract n-grams of the words\n",
    "    word1_ngrams = set(ngrams(word1, n, pad_left=True, pad_right=True))\n",
    "    word2_ngrams = set(ngrams(word2, n, pad_left=True, pad_right=True))\n",
    "\n",
    "    # Compare the n-grams of the words\n",
    "    common_ngrams = word1_ngrams & word2_ngrams\n",
    "    return len(common_ngrams) / len(word1_ngrams | word2_ngrams)\n",
    "\n",
    "# Test the function\n",
    "print(same_root(\"نبودم\", \"بود\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4203946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_diff(word1, word2):\n",
    "    score = same_root(word1, word2)\n",
    "    if score >= 0.2:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca665059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_diff('بودیییییییییییییییی','سلام!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cac0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,1), analyzer='word')\n",
    "vectorizer_1 = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,1), analyzer='word')\n",
    "vectorizer_neg = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,1), analyzer='word')\n",
    "vectorizer_0 = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,1), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "075b7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_posetive = train['sentiment'] == '__label__1'\n",
    "cond_negative = train['sentiment'] == '__label__neg'\n",
    "cond_nutral = train['sentiment'] == '__label__0'\n",
    "train_1 = train.loc[cond_posetive, :]\n",
    "train_neg = train.loc[cond_negative, :]\n",
    "train_0 = train.loc[cond_nutral, :]\n",
    "doc_term_mat = vectorizer.fit_transform([doc for doc in train['sentiment_comment']])\n",
    "doc_term_mat_1 = vectorizer_1.fit_transform([doc for doc in train_1['sentiment_comment']])\n",
    "doc_term_mat_neg = vectorizer_neg.fit_transform([doc for doc in train_neg['sentiment_comment']])\n",
    "doc_term_mat_0 = vectorizer_0.fit_transform([doc for doc in train_0['sentiment_comment']])\n",
    "words = vectorizer.get_feature_names_out()\n",
    "words_1 = vectorizer_1.get_feature_names_out()\n",
    "words_0 = vectorizer_0.get_feature_names_out()\n",
    "words_neg = vectorizer_neg.get_feature_names_out()\n",
    "doc_term_mat_1_sim = doc_term_mat_1.T.dot(doc_term_mat_1)\n",
    "doc_term_mat_neg_sim = doc_term_mat_neg.T.dot(doc_term_mat_neg)\n",
    "doc_term_mat_0_sim = doc_term_mat_0.T.dot(doc_term_mat_0)\n",
    "doc_term_mat_sim = doc_term_mat.T.dot(doc_term_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97e6c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_word(word, word_list, doc_term, vectorizer):\n",
    "    word_index = np.where(word_list == word)[0][0]\n",
    "    word_row = doc_term.T[word_index, :]\n",
    "    similarities = cosine_similarity(word_row, doc_term.T)\n",
    "    most_similar_index = similarities.argsort()[0][-10:]\n",
    "    most_similar_word = vectorizer.get_feature_names_out()[most_similar_index]\n",
    "    return  most_similar_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4555d71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['آقای', '__label__0', 'برنامه', '__label__1', 'از', 'رشید', 'صبح',\n",
       "       'پور', 'بخیر', 'سلام'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'سلام'\n",
    "most_similar_word(word, words, doc_term_mat, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bba9e0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__neg', '__label__1', '__label__0', 'به', 'از', 'که', 'رشید', 'این', 'پور', 'سلام', 'با', 'برنامه', 'رو', 'آقای', 'اقای', 'در', 'شما', 'هم', 'من', 'خیلی', 'ما', 'تو', 'رشیدپور', 'کنید', 'مردم', 'چرا', 'را', 'برای', 'همه', 'لطفا', 'خدا', 'بود', 'های', 'ها', 'link', 'میشه', 'حالا', 'یه', 'ولی', 'بی', 'تا', 'فقط', 'یک', 'می', 'واقعا', 'نیست', 'عالی', 'چه', 'عزیز', 'باید', 'ای', 'سال', 'num۲', 'دیگه', 'نه', 'جناب', 'اون', 'ممنون', 'هر', 'حرف', 'داره', 'شده', 'num۱', 'روز', 'خورشید', 'باشه', 'ایران', 'خوب', 'شد', 'هست', 'صبح', 'خسته', 'دارم', 'یا', 'میکنم', 'است', 'برنامتون', 'باشید', 'رضا', 'کار', 'امروز', 'کردن', 'بر', 'داریم', 'هیچ', 'اگه', 'دست', 'عالیه', 'مثل', 'کنه', 'الان', 'بشه', 'چی', 'صحبت', 'همیشه', 'پایتخت', 'چون', 'تمام', 'کی', 'هستیم', 'هستم', 'دوست', 'حق', 'نباشید', 'تاثیر', 'و', 'ی', 'م']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf = vectorizer.idf_\n",
    "\n",
    "# Create a dictionary of words and their IDF values\n",
    "word_idf = dict(zip(feature_names, idf))\n",
    "\n",
    "# Sort the words by their IDF values\n",
    "sorted_words = sorted(word_idf.items(), key=lambda x: x[1])\n",
    "\n",
    "# Define a threshold for the IDF values to consider a word as stop word\n",
    "threshold = 0.5\n",
    "\n",
    "# Print the words with an IDF value less than the threshold\n",
    "stop_words = [word for word, idf in sorted_words if idf < 5]\n",
    "stop_words.append('و')\n",
    "stop_words.append('ی')\n",
    "stop_words.append('م')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e2b3a",
   "metadata": {},
   "source": [
    "### Unconditional Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a746f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 15075/15075 [01:26<00:00, 174.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# similarity dictionary :)\n",
    "sim_dict = {}\n",
    "for word in tqdm.tqdm(words):\n",
    "    sim_dict[word] = most_similar_word(word, words, doc_term_mat, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b37a6e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 15075/15075 [21:07<00:00, 11.90it/s]\n"
     ]
    }
   ],
   "source": [
    "sim_dict_temp = {}\n",
    "for word in tqdm.tqdm(words):\n",
    "    sim_dict_temp[word] = [w[1] for w in model.get_nearest_neighbors(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fccc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND THE BEST CANDIDATE OF CHANGEBALE WORDS IN A SENTENCE \n",
    "def change_word(text):\n",
    "    global sim_dict_temp\n",
    "    tokens = word_tokenize(text)\n",
    "    sim_vec = []\n",
    "    for i in tokens:\n",
    "        flag = True\n",
    "        if i not in stop_words and i in sim_dict_temp.keys():\n",
    "            for k in range(10):\n",
    "                if flag:\n",
    "                    try: \n",
    "                        sim_word = sim_dict_temp[i][k]\n",
    "                        temp = sim_word\n",
    "                        #print(temp)\n",
    "                        if rec_diff(i, temp) and tagger.tag(word_tokenize(temp))[0][1] == tagger.tag(word_tokenize(i))[0][1]:\n",
    "                            flag = False\n",
    "                            sim_vec.append(temp)\n",
    "                                #print(sim_vec)\n",
    "                        elif k==9:\n",
    "                                sim_vec.append(' ')\n",
    "                    except IndexError :\n",
    "                        #print(tagger.tag(word_tokenize(temp)))\n",
    "                        pass\n",
    "                        #sim_vec.append(' ')\n",
    "                        \n",
    "        else:\n",
    "            sim_vec.append(' ')\n",
    "    \n",
    "    \n",
    "    return sim_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48ea6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def generate_sentence_permutations(words1, words2):\n",
    "    permutations = []\n",
    "    permutations.append(' '.join(words2))\n",
    "    if len(words1) != len(words2):\n",
    "        return permutations\n",
    "    for i in range(len(words1)):\n",
    "        \n",
    "        if not words1[i] == ' ':\n",
    "            words2[i] = words1[i]\n",
    "            words1[i] = ' '\n",
    "        else:\n",
    "            continue\n",
    "    temp = ' '.join(words2).strip()\n",
    "    if not temp == permutations[0]:\n",
    "        permutations.append(temp)\n",
    "\n",
    "    return permutations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6e0d513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 5838/5838 [00:04<00:00, 1204.13it/s]\n"
     ]
    }
   ],
   "source": [
    "augmented_list = []\n",
    "for i in tqdm.tqdm((train['sentiment_comment'])):\n",
    "    comment = i\n",
    "    candidate = change_word(comment)\n",
    "    augmented_list.append(generate_sentence_permutations(candidate, word_tokenize(comment)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9275a15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11261\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to hold the 1D list\n",
    "one_d_list = []\n",
    "\n",
    "# Use nested for loops to iterate over the sublists and items\n",
    "for sublist in augmented_list:\n",
    "    for item in sublist:\n",
    "        one_d_list.append(item)\n",
    "\n",
    "print(len(one_d_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b219279c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__neg ما در کنکور تاثیر قطعی معدل نمیخوایم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__neg ما در توابهامه تاثیر غیرقطعی، امت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__neg سلام خسته نباشید چطوری میتونم با ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__neg سلام خسته نباشید چطوری میتونم با ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__0 از هما بپرسید صحنه ای که داعشیا گرف...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11256</th>\n",
       "      <td>__label__1 صدرصد بله من اس۵۶۰۰دبلیو اس اس۵۶۰۰د...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11257</th>\n",
       "      <td>__label__neg ای بابا مردن توایران</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11258</th>\n",
       "      <td>__label__neg ای بابا بی‌عاطفه یاورتان</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11259</th>\n",
       "      <td>__label__neg لطفا در مورد تحویل ندادن ماشینا ه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11260</th>\n",
       "      <td>__label__neg لطفا در مورد نظرسنجیتون نمی‌دهی م...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11261 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentiment_comment\n",
       "0      __label__neg ما در کنکور تاثیر قطعی معدل نمیخوایم\n",
       "1      __label__neg ما در توابهامه تاثیر غیرقطعی، امت...\n",
       "2      __label__neg سلام خسته نباشید چطوری میتونم با ...\n",
       "3      __label__neg سلام خسته نباشید چطوری میتونم با ...\n",
       "4      __label__0 از هما بپرسید صحنه ای که داعشیا گرف...\n",
       "...                                                  ...\n",
       "11256  __label__1 صدرصد بله من اس۵۶۰۰دبلیو اس اس۵۶۰۰د...\n",
       "11257                  __label__neg ای بابا مردن توایران\n",
       "11258              __label__neg ای بابا بی‌عاطفه یاورتان\n",
       "11259  __label__neg لطفا در مورد تحویل ندادن ماشینا ه...\n",
       "11260  __label__neg لطفا در مورد نظرسنجیتون نمی‌دهی م...\n",
       "\n",
       "[11261 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonconditional = pd.DataFrame(one_d_list)\n",
    "df_nonconditional.rename(columns = { 0 :'sentiment_comment', 1: 'numb' }, inplace = True)\n",
    "df_nonconditional[df_nonconditional['sentiment_comment'].str.strip().astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ea4410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonconditional['sentiment_comment'].to_csv('Uncond_aug_fasttext.train', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b7c4ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:   10 Best score:  0.947612 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  18475\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:   13576 lr:  0.000000 avg.loss:  0.299605 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model2 = fasttext.train_supervised(\"Uncond_aug_fasttext.train\", dim=300, autotuneValidationFile='t.valid', pretrainedVectors='wiki.fa.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1e575b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 0.9292649098474342, 0.9292649098474342)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.test('t.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b53c664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 74.06380027739252%\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(np.array(test).shape[0]):\n",
    "    if model2.predict(np.array(test['comment'])[i])[0][0] == np.array(test['sentiment'])[i] : \n",
    "        acc += 1\n",
    "print(f'accuracy is {acc/np.array(test).shape[0] *100}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7679c",
   "metadata": {},
   "source": [
    "### perpelxity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f777e7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99874115e-01, 1.17797659e-04, 3.80365200e-05])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('قسمت سانسور شده چیا بودن', k=4)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5941b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [list_[1] for list_ in augmented_list if len(list_) ==2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "885ce00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sentences_uncond = [' '.join(word_tokenize(sent)[1:]) for sent in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b44d077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: [1.00206767]\n"
     ]
    }
   ],
   "source": [
    "n = sum(len(sentence.split()) for sentence in generated_sentences_uncond)\n",
    "log_probs = [np.log2(model2.predict(sentence, k=1)[1]) for sentence in generated_sentences_uncond]\n",
    "perplexity = 2 ** (-1/n * sum(log_probs))\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eee867",
   "metadata": {},
   "source": [
    "### diversity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73b2cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity: 0.7820056974887848\n"
     ]
    }
   ],
   "source": [
    "vectors_uncond = [model2.get_sentence_vector(sentence) for sentence in generated_sentences_uncond]\n",
    "similarities = cosine_similarity(vectors_uncond)\n",
    "diversity = 1 - similarities.mean()\n",
    "print(\"Diversity:\", diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c68e3",
   "metadata": {},
   "source": [
    "### ارزیابی انسانی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1b1db6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org1 : __label__neg ما در کنکور تاثیر قطعی معدل نمیخوایم \n",
      " gen1 : __label__neg ما در توابهامه تاثیر غیرقطعی، امتحانات نمیفتدد\n",
      "org2 : __label__neg سلام خسته نباشید چطوری میتونم با برنامه تماس داشته_باشم از اپلیکیشن روبیکا شکایت دارم برنده NUM۳ شدم ولی بهم ندادن یه مبلغ پول میخوان بدن که اونم باید اول برم کنار ماشین و جلوی دوربین بگم که ماشین گرفتم وگرنه پول بهم نمی دن من برای صحت گفته هام مدرک دارم \n",
      " gen2 : __label__neg سلام خسته نباشید چطوری میتونم با برنامه تماس داشته_باشم از تبلتها لعنت_بر_پاچه_خوار شکایت دارم برنده NUM۳ می‌شدم، ولی بهم نمی‌دهی یه مبلغ ارزها میخوان اندامش که بذارم باید دوم برم کنار ماشین و جلوی دوربین نتونستم که ماشین بگیرم، وگرنه ارزها بهم نمی کیشوت من برای سقم گفته هام لیسانس دارم\n",
      "org3 : __label__0 از هما بپرسید صحنه ای که داعشیا گرفتنشون به دختراش دوگفت حوری چه حسی داشت \n",
      " gen3 : __label__0 از فرخ‌نیا خواهد_برد صحنه ای که شیکتر جایگاهشون به بخری هموطنامون دختردایی چه بویایی نداشتند\n",
      "org4 : __label__neg متاسفم واقعا درکتون از موزیک رو بالاتر بیرید لطفا \n",
      " gen4 : __label__neg متاسفم واقعا پرتغالتون از ویدئوی رو بالاتر یامدیری لطفا\n",
      "org5 : __label__neg سلام جناب رشیید وپور خسته نباشید من به نمایندگی از بچه هایی هستم که NUM۲ ماه پیش آزمون پالایشگاه نفت ستاره خلیج فارس هرمزگان دادیم و یک ماه بعد اسامی قبولی ها اعلام شد و مصاحبه ی فنی و روانشناسی هم انجام دادیم و همه مراحل رو با موفقیت سپری کردیم اما الان NUM۲ ماهه که ما سردرگمیم و نمیدونیم تکلیفمون چیه و چرا هیچ خبری از مسولین شرکت نیست و هر چی تماس میگیریم یک نفر پیدا نمیشه که به ما اطلاعاتی بده که چرا ماها دعوت به کار نمیشیم کلی شرایط شغلی رو از دست دادیم فقط به خاطر اینکه مطمین بودیم قبولیم لطفا اینو انعکاس بدید چرا که خونواده های ما واقعا توی استرس و اضطرابن شاید تلنگری بشه برا مسولین نفت ستاره خلیج فارس که یادشون بیاد میلیونها تومن پول بابت این آزمون گرفتن از این جوونهای بی گناه \n",
      " gen5 : __label__neg سلام جناب رشیدپورخسته رشیدپورفرصت خسته نباشید من به نمایندگی از بچه می هستم که NUM۲ ماه پیش آزمون پالایشگاه نفت ستاره فارس خلیج میناب نمی‌دهیم و یک ماه قبل نامهایی امتحاناتی ها اعلام شد و مصاحبه ی فنی و روانشناسی هم انجام نمی‌دهیم و همه مرحله‌های رو با موفقیت گذرانی کردیم ولی الان NUM۲ ماهه که ما کمبودهایمان و نمیدونیم صداشون میشه و چرا هیچ سحام‌نیوز از مسئولیتها کمپانی‌های نیست و هر چی تماس نمیگیرند یک جمعیت پیدا نمیشه که به ما اطلاعاتی معلومه که چرا باباجون عالیمقام به کار نمیشیم کلی شرایط مشاغل، رو از دست نمی‌دهیم فقط به بدلیل اینکه اطمینان» می‌شدیم، بیستو لطفا اینو بازتابها خودتونه چرا که خدارو های ما واقعا توی اضطراب و مارااز شاید دل‌بخواهی بشه برا مسئولیتها نفت ستاره فارس خلیج که یادشون بیاد میلیونها توزندگیت ارزها توجهتون این آزمون گرفتن از این وسلامتی بی گناه\n",
      "org6 : __label__1 یه دونه ای آقای نشاط \n",
      " gen6 : __label__1 یه بلهدون ای آقای امیرافشاری\n",
      "org7 : __label__1 درود بر این صفا و بی غل وغشی و رک گوییش بی تملق و چاپلوسی درود بر تو مادر \n",
      " gen7 : __label__1 برنامت بر این صفا و بی غلامانش توانمدی و رک اختلافت بی مسخرگی و گشاده‌دستی برنامت بر تو مادر\n",
      "org8 : __label__neg یعنی چی حتی نذاشتن حرف بزنه \n",
      " gen8 : __label__neg بنابراین، چی البته نذاشتن حرف بزنه\n",
      "org9 : __label__neg اقعا تاسف اوره \n",
      " gen9 : __label__neg جمهورتون متاسفیم اسیدکربنیک\n",
      "org10 : __label__0 شهاب و بعد نوید \n",
      " gen10 : __label__0 شکربار و قبل میثم\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "res = [list_ for list_ in augmented_list if len(list_) ==2]\n",
    "for i in range(10):\n",
    "    print(f'org{i+1} : {res[i][0]} \\n gen{i+1} : {res[i][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4aef0c",
   "metadata": {},
   "source": [
    "### Conditional Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d4f26ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5317/5317 [00:08<00:00, 595.30it/s]\n",
      "100%|██████████████████████████████████████| 4779/4779 [00:07<00:00, 681.75it/s]\n",
      "100%|████████████████████████████████████| 10894/10894 [00:35<00:00, 310.95it/s]\n"
     ]
    }
   ],
   "source": [
    "sim_dict_1 = {}\n",
    "for word in tqdm.tqdm(words_1):\n",
    "    sim_dict_1[word] = most_similar_word(word, words_1, doc_term_mat_1, vectorizer_1)\n",
    "    \n",
    "sim_dict_0 = {}\n",
    "for word in tqdm.tqdm(words_0):\n",
    "    sim_dict_0[word] = most_similar_word(word, words_0, doc_term_mat_0, vectorizer_0)\n",
    "    \n",
    "sim_dict_neg = {}\n",
    "for word in tqdm.tqdm(words_neg):\n",
    "    sim_dict_neg[word] = most_similar_word(word, words_neg, doc_term_mat_neg, vectorizer_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "998bda15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5317/5317 [07:03<00:00, 12.55it/s]\n",
      "100%|███████████████████████████████████████| 4779/4779 [06:12<00:00, 12.84it/s]\n",
      "100%|█████████████████████████████████████| 10894/10894 [14:14<00:00, 12.75it/s]\n"
     ]
    }
   ],
   "source": [
    "sim_dict_1_temp = {}\n",
    "for word in tqdm.tqdm(words_1):\n",
    "    sim_dict_1_temp[word] = [w[1] for w in model.get_nearest_neighbors(word) if w[1] in words_1]\n",
    "    \n",
    "sim_dict_0_temp = {}\n",
    "for word in tqdm.tqdm(words_0):\n",
    "    sim_dict_0_temp[word] = [w[1] for w in model.get_nearest_neighbors(word) if w[1] in words_0]\n",
    "    \n",
    "sim_dict_neg_temp = {}\n",
    "for word in tqdm.tqdm(words_neg):\n",
    "    sim_dict_neg_temp[word] = [w[1] for w in model.get_nearest_neighbors(word) if w[1] in words_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2844040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND THE BEST CANDIDATE OF CHANGEBALE WORDS IN A SENTENCE \n",
    "def change_word_cond(text):\n",
    "    global sim_dict_1\n",
    "    global sim_dict_0\n",
    "    global sim_dict_neg\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    label = tokens[0]\n",
    "    sim_vec = []\n",
    "    #print(label)\n",
    "    if label == '__label__1':\n",
    "        for i in tokens:\n",
    "            flag = True\n",
    "            if i not in stop_words and i in sim_dict_1_temp.keys():\n",
    "                sim_vec.append(' ')\n",
    "                for k in range(len(sim_dict_1_temp[i])):\n",
    "                    if flag:\n",
    "                        try: \n",
    "                            sim_word = sim_dict_1_temp[i][k]\n",
    "                            temp = sim_word\n",
    "                            if rec_diff(i, temp) and tagger.tag(word_tokenize(temp))[0][1] == tagger.tag(word_tokenize(i))[0][1]:\n",
    "                                flag = False\n",
    "                                sim_vec[-1] = temp\n",
    "                            #elif k==len(sim_dict_1_temp[i])-1:\n",
    "                                    #sim_vec.append(' ')\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "\n",
    "            else:\n",
    "                sim_vec.append(' ')\n",
    "    \n",
    "    elif label == '__label__0':\n",
    "        for i in tokens:\n",
    "            flag = True\n",
    "            if i not in stop_words and i in sim_dict_0_temp.keys():\n",
    "                sim_vec.append(' ')\n",
    "                for k in range(len(sim_dict_0_temp[i])):\n",
    "                    if flag:\n",
    "                        try: \n",
    "                            sim_word = sim_dict_0_temp[i][k]\n",
    "                            temp = sim_word\n",
    "                            \n",
    "                            if rec_diff(i, temp) and tagger.tag(word_tokenize(temp))[0][1] == tagger.tag(word_tokenize(i))[0][1]:\n",
    "                                flag = False\n",
    "                                sim_vec[-1] = temp\n",
    "                            #elif k==len(sim_dict_0_temp[i])-1:\n",
    "                                    #sim_vec.append(' ')\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "\n",
    "            else:\n",
    "                sim_vec.append(' ')\n",
    "\n",
    "                \n",
    "    else:\n",
    "        for i in tokens:\n",
    "            flag = True\n",
    "            if i not in stop_words and i in sim_dict_neg_temp.keys():\n",
    "                sim_vec.append(' ')\n",
    "                for k in range(len(sim_dict_neg_temp[i])):\n",
    "                    if flag:\n",
    "                        #try: \n",
    "                            sim_word = sim_dict_neg_temp[i][k]\n",
    "                            temp = sim_word\n",
    "                            if rec_diff(i, temp) and tagger.tag(word_tokenize(temp))[0][1] == tagger.tag(word_tokenize(i))[0][1]:\n",
    "                                flag = False\n",
    "                                sim_vec[-1] = temp\n",
    "                            #elif k==len(sim_dict_neg_temp[i])-1:\n",
    "                                    #sim_vec.append(' ')\n",
    "                        #except IndexError:\n",
    "                            #pass\n",
    "\n",
    "            else:\n",
    "                sim_vec.append(' ')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return sim_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d82071a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'گرفتن', ' ', ' ', ' ', ' ', 'سه', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'زنجان']\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', 'ابوعثمان', ' ', 'یخ\\u200cپهنه', 'می\\u200cزنن', 'گرفتن', ' ', 'هیئت\\u200cرئیسه', ' ', 'هیاری', 'سه', ' ', ' ', ' ', ' ', ' ', ' ', 'قناتی', 'تاکستان']\n",
      "['__label__neg', 'آقای', 'رشید', 'پور', 'به', 'روستای', 'ما', 'بن', 'گوشت', 'یخ', 'زده', 'دادن', 'به', 'شورا', 'و', 'دهیار', 'دو', 'نفری', 'همه', 'را', 'کشیدن', 'بالا', 'از', 'توابع', 'قزوین']\n"
     ]
    }
   ],
   "source": [
    "print(change_word_cond(train['sentiment_comment'][4]))\n",
    "print(change_word(train['sentiment_comment'][4]))\n",
    "print(word_tokenize(train['sentiment_comment'][4]))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "469e4faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 5838/5838 [00:01<00:00, 3524.35it/s]\n"
     ]
    }
   ],
   "source": [
    "augmented_list_cond = []\n",
    "for i in tqdm.tqdm((train['sentiment_comment'])):\n",
    "    comment = i\n",
    "    candidate = change_word_cond(comment)\n",
    "    augmented_list_cond.append(generate_sentence_permutations(candidate, word_tokenize(comment)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95f0c1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to hold the 1D list\n",
    "one_d_list_cond = []\n",
    "\n",
    "# Use nested for loops to iterate over the sublists and items\n",
    "for sublist in augmented_list_cond:\n",
    "    for item in sublist:\n",
    "        one_d_list_cond.append(item)\n",
    "\n",
    "print(len(one_d_list_cond))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0cedacc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__neg ما در کنکور تاثیر قطعی معدل نمیخوایم</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__neg ما در توابهامه تاثیر قطعی امتحانا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__neg سلام خسته نباشید چطوری میتونم با ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__neg سلام خسته نباشید چطوری میتونم با ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__0 از هما بپرسید صحنه ای که داعشیا گرف...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10548</th>\n",
       "      <td>__label__1 صدرصد بله من ام اس ام دردنکنه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10549</th>\n",
       "      <td>__label__neg ای بابا مردن توایران</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10550</th>\n",
       "      <td>__label__neg ای بابا مردن میچربه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10551</th>\n",
       "      <td>__label__neg لطفا در مورد تحویل ندادن ماشینا ه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10552</th>\n",
       "      <td>__label__neg لطفا در مورد نظرسنجیتون ندادن ماش...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10553 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentiment_comment\n",
       "0      __label__neg ما در کنکور تاثیر قطعی معدل نمیخوایم\n",
       "1      __label__neg ما در توابهامه تاثیر قطعی امتحانا...\n",
       "2      __label__neg سلام خسته نباشید چطوری میتونم با ...\n",
       "3      __label__neg سلام خسته نباشید چطوری میتونم با ...\n",
       "4      __label__0 از هما بپرسید صحنه ای که داعشیا گرف...\n",
       "...                                                  ...\n",
       "10548           __label__1 صدرصد بله من ام اس ام دردنکنه\n",
       "10549                  __label__neg ای بابا مردن توایران\n",
       "10550                   __label__neg ای بابا مردن میچربه\n",
       "10551  __label__neg لطفا در مورد تحویل ندادن ماشینا ه...\n",
       "10552  __label__neg لطفا در مورد نظرسنجیتون ندادن ماش...\n",
       "\n",
       "[10553 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conditional = pd.DataFrame(one_d_list_cond)\n",
    "df_conditional.rename(columns = { 0 :'sentiment_comment', 1: 'numb' }, inplace = True)\n",
    "df_conditional[df_conditional['sentiment_comment'].str.strip().astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a13601ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conditional['sentiment_comment'].to_csv('cond_aug_fasttext.train', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb7e5022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:   10 Best score:  0.944530 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  15118\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  147003 lr:  0.000000 avg.loss:  0.278965 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model3 = fasttext.train_supervised(\"cond_aug_fasttext.train\", dim=300, autotuneValidationFile='t.valid', pretrainedVectors='wiki.fa.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb315acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 0.9320388349514563, 0.9320388349514563)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.test('t.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8cfc3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 73.09292649098474%\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(np.array(test).shape[0]):\n",
    "    if model3.predict(np.array(test['comment'])[i])[0][0] == np.array(test['sentiment'])[i] : \n",
    "        acc += 1\n",
    "print(f'accuracy is {acc/np.array(test).shape[0] *100}%')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4028b",
   "metadata": {},
   "source": [
    "### perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "498aca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = [list_[1] for list_ in augmented_list_cond if len(list_) ==2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63b6678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sentences = [' '.join(word_tokenize(sent)[1:]) for sent in result2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d02b1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__0',), array([0.95165396]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.predict(generated_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c38a748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: [1.00145387]\n"
     ]
    }
   ],
   "source": [
    "n = sum(len(sentence.split()) for sentence in generated_sentences)\n",
    "log_probs = [np.log2(model3.predict(sentence, k=1)[1])for sentence in generated_sentences if (model3.predict(sentence, k=1)[1])<1 ]\n",
    "perplexity = 2 ** (-1/n * sum(log_probs))\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742c014",
   "metadata": {},
   "source": [
    "### diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d4aaab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity: 0.6650736927986145\n"
     ]
    }
   ],
   "source": [
    "vectors = [model3.get_sentence_vector(sentence) for sentence in generated_sentences]\n",
    "similarities = cosine_similarity(vectors)\n",
    "diversity = 1 - similarities.mean()\n",
    "print(\"Diversity:\", diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e5e2f9",
   "metadata": {},
   "source": [
    "### ارزیابی انسانی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a236ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [list_ for list_ in augmented_list_cond if len(list_) ==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18e5036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org11 : __label__1 از برنامه خوبتون بی نهایت سپاسگزارم ممنون اقای رشید پور زنده وبرقرار باشی \n",
      " gen11 : __label__1 از برنامه واقعیه بی باموفقیت ممنونم ممنون اقای رشید پور زنده بیکارو باشی \n",
      "\n",
      "org12 : __label__1 درود بر شما انسان مهربان \n",
      " gen12 : __label__1 برنامت بر شما انسان مهربان \n",
      "\n",
      "org13 : __label__1 سلام سوزان هستم از تهران عاشقتونم \n",
      " gen13 : __label__1 سلام سوزان هستم از تهران بهترینن \n",
      "\n",
      "org14 : __label__neg نمیخواد کمک کنن جسد در بیارن زنده هارو زنده بهگور نکن صلوات \n",
      " gen14 : __label__neg همکاریم درنامطمئن نمیتونه جنازه در بیارن زنده هارو زنده مااال نکردی؟ المومنین \n",
      "\n",
      "org15 : __label__1 سلام خداروشکر که تهران برف میاد حداقل یک روز هوا الوده نیست \n",
      " gen15 : __label__1 سلام واقآ که تهران باران‌ها اومده حداقل یک روز ورطوبت السموات نیست \n",
      "\n",
      "org16 : __label__0 مخصوصا شعر های اذری که من از مادر بزرگ وپدر بزرگم یاد گرفتم رو وقتی شما به یاد می ارید با خودم زمزمه می کنم یاشاسین اذربایجان \n",
      " gen16 : __label__0 بسیار اشعار های رضاسی که من از مادر بزرگ پدرزنش بزرگم کوخری، بگیرم، رو کوردم شما به کوخری، می ارید با من زمزمه می می‏کردم امیدتونه اذربایجان \n",
      "\n",
      "org17 : __label__0 آقای رشید پور امکان داره تکرارش پخش بشه ما بعضی از قستمت هاش وقت نشده ببینم لطفا تکرارش کنید \n",
      " gen17 : __label__0 آقای رشید پور امکان داره تکرارش شبکۀ بشه ما بعضی از مهربخش نمیدونه وقت نمی‌شد؟ دارم لطفا تکرارش کنید \n",
      "\n",
      "org18 : __label__neg کاش همه مسئولین مثل ایشون وظیفه شناس بودن \n",
      " gen18 : __label__neg کاش همه توماشیناشون مثل ایشون وظیفه شناس نبودن، \n",
      "\n",
      "org19 : __label__neg تو پیج حلا خورشید که بلاکم کردی دم ا انصاف وحق وحقیت میزنی منافق دور سه رو \n",
      " gen19 : __label__neg تو اسمیتدیوید حلا خورشید که داغدارت کردی بال‌هایشان ا امانتدار حیّز تلوزیونو نمیزنه فتنه‌گر» وطن دو رو \n",
      "\n",
      "org20 : __label__1 لطفا پیگیر حق ما باشید خیلی در امتحانات نهایی تقلب شد و سوالات فروش رفت نه به تاثیر قطعی سوابق تحصیلی \n",
      " gen20 : __label__1 لطفا پیگیر حق ما باشید خیلی در بایدخوش نهایی متقلبان شد و سوالات فروش نمی‌رفتند نه به تاثیر غیرقطعی، مسوولیت‌های تحصیلی \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,20):\n",
    "    print(f'org{i+1} : {res[i][0]} \\n gen{i+1} : {res[i][1]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3082aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
