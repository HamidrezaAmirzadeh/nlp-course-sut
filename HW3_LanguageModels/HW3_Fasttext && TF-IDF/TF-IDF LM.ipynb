{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a7d94f",
   "metadata": {},
   "source": [
    "# جهت اجرای نوت‌بوک لطفا فایل زیر را دانلود کنید و در فولدر مربوط به همین فایل قرار دهید.\n",
    "\n",
    "\n",
    "https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.fa.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2b185",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bbf2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "import fasttext\n",
    "from hazm import *\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tqdm\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563ed456",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4222e553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    2975\n",
       " 1    2354\n",
       " 0    1879\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pre-proc-data2.csv')\n",
    "text = df['comment']\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c8b25",
   "metadata": {},
   "source": [
    "### Preparing the dataset to be compatible with the fasttext structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379b0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(-1 , 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b36eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_tok</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>آقای رشیدپور من تا هفته NUM۲ بچه‌ام پسر بود و ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>چقدر این دزد باحال بود</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>سلام اگر ممکنه از کلمه سی تی اسکن تو برنامتون ...</td>\n",
       "      <td>__label__neg</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            comment  \\\n",
       "0           0  آقای رشیدپور من تا هفته NUM۲ بچه‌ام پسر بود و ...   \n",
       "1           1                            چقدر این دزد باحال بود    \n",
       "2           2  سلام اگر ممکنه از کلمه سی تی اسکن تو برنامتون ...   \n",
       "\n",
       "      sentiment  num_tok  \n",
       "0    __label__1       31  \n",
       "1    __label__1        5  \n",
       "2  __label__neg       19  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = \"__label__\" + df['sentiment'].astype(str)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a37f019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_tok</th>\n",
       "      <th>sentiment_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>آقای رشیدپور من تا هفته NUM۲ بچه‌ام پسر بود و ...</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>31</td>\n",
       "      <td>__label__1 آقای رشیدپور من تا هفته NUM۲ بچه‌ام...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>چقدر این دزد باحال بود</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>5</td>\n",
       "      <td>__label__1 چقدر این دزد باحال بود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>سلام اگر ممکنه از کلمه سی تی اسکن تو برنامتون ...</td>\n",
       "      <td>__label__neg</td>\n",
       "      <td>19</td>\n",
       "      <td>__label__neg سلام اگر ممکنه از کلمه سی تی اسکن...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            comment  \\\n",
       "0           0  آقای رشیدپور من تا هفته NUM۲ بچه‌ام پسر بود و ...   \n",
       "1           1                            چقدر این دزد باحال بود    \n",
       "2           2  سلام اگر ممکنه از کلمه سی تی اسکن تو برنامتون ...   \n",
       "\n",
       "      sentiment  num_tok                                  sentiment_comment  \n",
       "0    __label__1       31  __label__1 آقای رشیدپور من تا هفته NUM۲ بچه‌ام...  \n",
       "1    __label__1        5                 __label__1 چقدر این دزد باحال بود   \n",
       "2  __label__neg       19  __label__neg سلام اگر ممکنه از کلمه سی تی اسکن...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment_comment'] = df['sentiment'] +' '+ df['comment']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23972d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre(text):\n",
    "    text = re.sub(r'[^\\w\\s]',' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e52bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_comment'] = [pre(str(i)) for i in df['sentiment_comment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb350e81",
   "metadata": {},
   "source": [
    "### split dataset to train, test and valid sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8493c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.1, shuffle=True)\n",
    "train , valid = train_test_split(train, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b272c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"t.train\", columns= ['sentiment_comment'], index = False, header = False)\n",
    "test.to_csv(\"t.test\", columns= ['sentiment_comment'], index = False, header = False)\n",
    "valid.to_csv(\"t.valid\", columns= ['sentiment_comment'], index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803edf97",
   "metadata": {},
   "source": [
    "### train a model using fasttext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "054a5715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:   10 Best score:  0.764253 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  14904\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  103793 lr:  0.000000 avg.loss:  0.417233 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(\"t.train\", dim=300, autotuneValidationFile='t.valid', pretrainedVectors='wiki.fa.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39aa563a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 0.7253814147018031, 0.7253814147018031)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('t.test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183eef29",
   "metadata": {},
   "source": [
    "### Accuracy without Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f06a68d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 72.5381414701803%\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(np.array(test).shape[0]):\n",
    "    if model.predict(np.array(test['comment'])[i])[0][0] == np.array(test['sentiment'])[i] : \n",
    "        acc += 1\n",
    "print(f'accuracy is {acc/np.array(test).shape[0] *100}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7584abd",
   "metadata": {},
   "source": [
    "### -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7d38f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "stemmer = Stemmer()\n",
    "tagger = POSTagger(model='resources-0/postagger.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c95cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "def same_root(word1, word2, n=3):\n",
    "    # Extract n-grams of the words\n",
    "    word1_ngrams = set(ngrams(word1, n, pad_left=True, pad_right=True))\n",
    "    word2_ngrams = set(ngrams(word2, n, pad_left=True, pad_right=True))\n",
    "\n",
    "    # Compare the n-grams of the words\n",
    "    common_ngrams = word1_ngrams & word2_ngrams\n",
    "    return len(common_ngrams) / len(word1_ngrams | word2_ngrams)\n",
    "\n",
    "# Test the function\n",
    "print(same_root(\"نبودم\", \"بود\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4203946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_diff(word1, word2):\n",
    "    score = same_root(word1, word2)\n",
    "    if score >= 0.2:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca665059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_diff('بودیییییییییییییییی','سلام!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cac0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,1), analyzer='word')\n",
    "vectorizer_1 = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,1), analyzer='word')\n",
    "vectorizer_neg = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,1), analyzer='word')\n",
    "vectorizer_0 = TfidfVectorizer(use_idf = True, norm ='l2', ngram_range=(1,1), analyzer='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "075b7b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_posetive = train['sentiment'] == '__label__1'\n",
    "cond_negative = train['sentiment'] == '__label__neg'\n",
    "cond_nutral = train['sentiment'] == '__label__0'\n",
    "train_1 = train.loc[cond_posetive, :]\n",
    "train_neg = train.loc[cond_negative, :]\n",
    "train_0 = train.loc[cond_nutral, :]\n",
    "doc_term_mat = vectorizer.fit_transform([doc for doc in train['sentiment_comment']])\n",
    "doc_term_mat_1 = vectorizer_1.fit_transform([doc for doc in train_1['sentiment_comment']])\n",
    "doc_term_mat_neg = vectorizer_neg.fit_transform([doc for doc in train_neg['sentiment_comment']])\n",
    "doc_term_mat_0 = vectorizer_0.fit_transform([doc for doc in train_0['sentiment_comment']])\n",
    "words = vectorizer.get_feature_names_out()\n",
    "words_1 = vectorizer_1.get_feature_names_out()\n",
    "words_0 = vectorizer_0.get_feature_names_out()\n",
    "words_neg = vectorizer_neg.get_feature_names_out()\n",
    "doc_term_mat_1_sim = doc_term_mat_1.T.dot(doc_term_mat_1)\n",
    "doc_term_mat_neg_sim = doc_term_mat_neg.T.dot(doc_term_mat_neg)\n",
    "doc_term_mat_0_sim = doc_term_mat_0.T.dot(doc_term_mat_0)\n",
    "doc_term_mat_sim = doc_term_mat.T.dot(doc_term_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97e6c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_word(word, word_list, doc_term, vectorizer):\n",
    "    word_index = np.where(word_list == word)[0][0]\n",
    "    word_row = doc_term.T[word_index, :]\n",
    "    similarities = cosine_similarity(word_row, doc_term.T)\n",
    "    most_similar_index = similarities.argsort()[0][-10:]\n",
    "    most_similar_word = vectorizer.get_feature_names_out()[most_similar_index]\n",
    "    return  most_similar_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4555d71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['خسته', 'آقای', 'برنامه', 'از', '__label__1', 'رشید', 'پور', 'صبح',\n",
       "       'بخیر', 'سلام'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'سلام'\n",
    "most_similar_word(word, words, doc_term_mat, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bba9e0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__neg', '__label__1', '__label__0', 'به', 'از', 'که', 'رشید', 'پور', 'این', 'سلام', 'با', 'رو', 'برنامه', 'آقای', 'اقای', 'در', 'شما', 'هم', 'من', 'خیلی', 'ما', 'تو', 'رشیدپور', 'مردم', 'کنید', 'چرا', 'همه', 'را', 'برای', 'خدا', 'لطفا', 'بود', 'های', 'حالا', 'link', 'ها', 'یه', 'میشه', 'تا', 'ولی', 'فقط', 'بی', 'یک', 'عالی', 'می', 'واقعا', 'چه', 'نیست', 'ای', 'عزیز', 'دیگه', 'باید', 'خورشید', 'سال', 'num۲', 'هر', 'شده', 'روز', 'اون', 'جناب', 'داره', 'num۱', 'نه', 'حرف', 'باشه', 'ممنون', 'ایران', 'هست', 'خوب', 'دارم', 'صبح', 'یا', 'خسته', 'شد', 'میکنم', 'کار', 'برنامتون', 'است', 'رضا', 'کردن', 'باشید', 'هیچ', 'اگه', 'بشه', 'دست', 'عالیه', 'چی', 'همیشه', 'بر', 'کنه', 'داریم', 'صحبت', 'امروز', 'مثل', 'کی', 'بعد', 'پایتخت', 'چون', 'الان', 'دوست', 'اگر', 'تمام', 'تون', 'هستیم', 'نباشید', 'هستم', 'کم', 'برف', 'و', 'ی', 'م']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "idf = vectorizer.idf_\n",
    "\n",
    "# Create a dictionary of words and their IDF values\n",
    "word_idf = dict(zip(feature_names, idf))\n",
    "\n",
    "# Sort the words by their IDF values\n",
    "sorted_words = sorted(word_idf.items(), key=lambda x: x[1])\n",
    "\n",
    "# Define a threshold for the IDF values to consider a word as stop word\n",
    "threshold = 0.5\n",
    "\n",
    "# Print the words with an IDF value less than the threshold\n",
    "stop_words = [word for word, idf in sorted_words if idf < 5]\n",
    "stop_words.append('و')\n",
    "stop_words.append('ی')\n",
    "stop_words.append('م')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e2b3a",
   "metadata": {},
   "source": [
    "### Unconditional Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a746f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 14863/14863 [01:41<00:00, 146.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# similarity dictionary :)\n",
    "sim_dict = {}\n",
    "for word in tqdm.tqdm(words):\n",
    "    sim_dict[word] = most_similar_word(word, words, doc_term_mat, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fccc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND THE BEST CANDIDATE OF CHANGEBALE WORDS IN A SENTENCE \n",
    "def change_word(text):\n",
    "    global sim_dict\n",
    "    tokens = word_tokenize(text)\n",
    "    sim_vec = []\n",
    "    for i in tokens:\n",
    "        flag = True\n",
    "        if i not in stop_words and i in sim_dict.keys():\n",
    "            for k in range(7):\n",
    "                if flag:\n",
    "                    try: \n",
    "                        sim_word = sim_dict[i][k]\n",
    "                        temp = sim_word\n",
    "                        #print(temp)\n",
    "                        if rec_diff(i, temp) and tagger.tag(word_tokenize(temp))[0][1] == tagger.tag(word_tokenize(i))[0][1]:\n",
    "                            flag = False\n",
    "                            sim_vec.append(temp)\n",
    "                                #print(sim_vec)\n",
    "                        elif k==9:\n",
    "                                sim_vec.append(' ')\n",
    "                    except IndexError :\n",
    "                        #print(tagger.tag(word_tokenize(temp)))\n",
    "                        pass\n",
    "                        #sim_vec.append(' ')\n",
    "                        \n",
    "        else:\n",
    "            sim_vec.append(' ')\n",
    "    \n",
    "    \n",
    "    return sim_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48ea6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def generate_sentence_permutations(words1, words2):\n",
    "    permutations = []\n",
    "    permutations.append(' '.join(words2))\n",
    "    if len(words1) != len(words2):\n",
    "        return permutations\n",
    "    for i in range(len(words1)):\n",
    "        \n",
    "        if not words1[i] == ' ':\n",
    "            words2[i] = words1[i]\n",
    "            words1[i] = ' '\n",
    "        else:\n",
    "            continue\n",
    "    temp = ' '.join(words2).strip()\n",
    "    if not temp == permutations[0]:\n",
    "        permutations.append(temp)\n",
    "\n",
    "    return permutations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6e0d513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 5838/5838 [00:05<00:00, 1065.65it/s]\n"
     ]
    }
   ],
   "source": [
    "augmented_list = []\n",
    "for i in tqdm.tqdm((train['sentiment_comment'])):\n",
    "    comment = i\n",
    "    candidate = change_word(comment)\n",
    "    augmented_list.append(generate_sentence_permutations(candidate, word_tokenize(comment)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9275a15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7306\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to hold the 1D list\n",
    "one_d_list = []\n",
    "\n",
    "# Use nested for loops to iterate over the sublists and items\n",
    "for sublist in augmented_list:\n",
    "    for item in sublist:\n",
    "        one_d_list.append(item)\n",
    "\n",
    "print(len(one_d_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b219279c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__neg از محمد اصفهانی دعوت نکنین</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__neg از ونوید نکنین خوانندهدها قضانمیشه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__1 سلام با تشکر فراوان از برنامه پر ان...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__neg رشیدپور از چشم همه افتاده نون به ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__0 سالار عقیلی شماره ۶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7301</th>\n",
       "      <td>__label__neg مسوولان در حال شمردن پولاشونن وقت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7302</th>\n",
       "      <td>__label__neg چطوری سوپاپ اطمینان کی میخوای خجا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7303</th>\n",
       "      <td>__label__0 به آقا تنابنده بگید یه تیکه شمالی ح...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7304</th>\n",
       "      <td>__label__0 به نگار ازآقای بسازند یه سانسور جدی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7305</th>\n",
       "      <td>__label__1 اقای رضا رشید پور عزیز</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7306 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sentiment_comment\n",
       "0               __label__neg از محمد اصفهانی دعوت نکنین\n",
       "1       __label__neg از ونوید نکنین خوانندهدها قضانمیشه\n",
       "2     __label__1 سلام با تشکر فراوان از برنامه پر ان...\n",
       "3     __label__neg رشیدپور از چشم همه افتاده نون به ...\n",
       "4                        __label__0 سالار عقیلی شماره ۶\n",
       "...                                                 ...\n",
       "7301  __label__neg مسوولان در حال شمردن پولاشونن وقت...\n",
       "7302  __label__neg چطوری سوپاپ اطمینان کی میخوای خجا...\n",
       "7303  __label__0 به آقا تنابنده بگید یه تیکه شمالی ح...\n",
       "7304  __label__0 به نگار ازآقای بسازند یه سانسور جدی...\n",
       "7305                  __label__1 اقای رضا رشید پور عزیز\n",
       "\n",
       "[7306 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonconditional = pd.DataFrame(one_d_list)\n",
    "df_nonconditional.rename(columns = { 0 :'sentiment_comment', 1: 'numb' }, inplace = True)\n",
    "df_nonconditional[df_nonconditional['sentiment_comment'].str.strip().astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ea4410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonconditional['sentiment_comment'].to_csv('Uncond_aug_TFIDF.train', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b7c4ca2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:   11 Best score:  0.767334 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  14908\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  146940 lr:  0.000000 avg.loss:  0.506957 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model2 = fasttext.train_supervised(\"Uncond_aug_TFIDF.train\", dim=300, autotuneValidationFile='t.valid', pretrainedVectors='wiki.fa.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1e575b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 0.723994452149792, 0.723994452149792)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.test('t.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b53c664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 73.09292649098474%\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(np.array(test).shape[0]):\n",
    "    if model2.predict(np.array(test['comment'])[i])[0][0] == np.array(test['sentiment'])[i] : \n",
    "        acc += 1\n",
    "print(f'accuracy is {acc/np.array(test).shape[0] *100}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7679c",
   "metadata": {},
   "source": [
    "### perpelxity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f777e7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87191868, 0.12578608, 0.00232524])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict('قسمت سانسور شده چیا بودن', k=4)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5941b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [list_[1] for list_ in augmented_list if len(list_) ==2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "885ce00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sentences_uncond = [' '.join(word_tokenize(sent)[1:]) for sent in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b44d077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: [1.03191698]\n"
     ]
    }
   ],
   "source": [
    "n = sum(len(sentence.split()) for sentence in generated_sentences_uncond)\n",
    "log_probs = [np.log2(model2.predict(sentence, k=1)[1]) for sentence in generated_sentences_uncond]\n",
    "perplexity = 2 ** (-1/n * sum(log_probs))\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eee867",
   "metadata": {},
   "source": [
    "### diversity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73b2cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity: 0.6769970655441284\n"
     ]
    }
   ],
   "source": [
    "vectors_uncond = [model2.get_sentence_vector(sentence) for sentence in generated_sentences_uncond]\n",
    "similarities = cosine_similarity(vectors_uncond)\n",
    "diversity = 1 - similarities.mean()\n",
    "print(\"Diversity:\", diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea45567e",
   "metadata": {},
   "source": [
    "### ارزیابی انسانی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98f32f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org1 : __label__neg از محمد اصفهانی دعوت نکنین \n",
      " gen1 : __label__neg از ونوید نکنین خوانندهدها قضانمیشه \n",
      "\n",
      "org2 : __label__0 سالار عقیلی شماره ۶ \n",
      " gen2 : __label__0 چاووشی اسطوره smsدادم ۶ \n",
      "\n",
      "org3 : __label__1 یزد خودونم برف أومده إقه خش شده \n",
      " gen3 : __label__1 امیر برف برف خودشونن خودشونن خودشونن شده \n",
      "\n",
      "org4 : __label__0 مناظره بین دکتر سبطی وآقای بطحایی \n",
      " gen4 : __label__0 ودکتر ودکتر بدقولی آقائی نشاط ازقول \n",
      "\n",
      "org5 : __label__neg برنامه تون و تمام نکنید لطفا \n",
      " gen5 : __label__neg برنامه تون و تمام وجدانند لطفا \n",
      "\n",
      "org6 : __label__0 قسمت سانسور شده چیا بودن \n",
      " gen6 : __label__0 کامیون تیکه شده خودشن همسرشون \n",
      "\n",
      "org7 : __label__0 همه بازماندگانی که سال NUM۲ سلامت به پایان رساندن \n",
      " gen7 : __label__0 همه سال که سال NUM۲ درصدف به انتظارمان سال \n",
      "\n",
      "org8 : __label__0 محسن از مبا \n",
      " gen8 : __label__0 شحریان از خودشون \n",
      "\n",
      "org9 : __label__0 استاد امیر بیات \n",
      " gen9 : __label__0 عاشقتیم دهنت خودشمااقای \n",
      "\n",
      "org10 : __label__0 مناظره باوزیر آموزش وپرورش \n",
      " gen10 : __label__0 ودکتر خودشونو دارایی خودشونو \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "res = [list_ for list_ in augmented_list if len(list_) ==2]\n",
    "for i in range(10):\n",
    "    print(f'org{i+1} : {res[i][0]} \\n gen{i+1} : {res[i][1]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4aef0c",
   "metadata": {},
   "source": [
    "### Conditional Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d4f26ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5345/5345 [00:10<00:00, 495.74it/s]\n",
      "100%|██████████████████████████████████████| 4747/4747 [00:08<00:00, 580.84it/s]\n",
      "100%|████████████████████████████████████| 10669/10669 [00:45<00:00, 236.84it/s]\n"
     ]
    }
   ],
   "source": [
    "sim_dict_1 = {}\n",
    "for word in tqdm.tqdm(words_1):\n",
    "    sim_dict_1[word] = most_similar_word(word, words_1, doc_term_mat_1, vectorizer_1)\n",
    "    \n",
    "sim_dict_0 = {}\n",
    "for word in tqdm.tqdm(words_0):\n",
    "    sim_dict_0[word] = most_similar_word(word, words_0, doc_term_mat_0, vectorizer_0)\n",
    "    \n",
    "sim_dict_neg = {}\n",
    "for word in tqdm.tqdm(words_neg):\n",
    "    sim_dict_neg[word] = most_similar_word(word, words_neg, doc_term_mat_neg, vectorizer_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2844040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND THE BEST CANDIDATE OF CHANGEBALE WORDS IN A SENTENCE \n",
    "def change_word_cond(text):\n",
    "    global sim_dict_1\n",
    "    global sim_dict_0\n",
    "    global sim_dict_neg\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    label = tokens[0]\n",
    "    sim_vec = []\n",
    "    #print(label)\n",
    "    if label == '__label__1':\n",
    "        for i in tokens:\n",
    "            flag = True\n",
    "            if i not in stop_words and i in sim_dict_1.keys():\n",
    "                for k in range(7):\n",
    "                    if flag:\n",
    "                        try: \n",
    "                            sim_word = sim_dict_1[i][k]\n",
    "                            temp = sim_word\n",
    "                            if rec_diff(i, temp) and tagger.tag(word_tokenize(temp))[0][1] == tagger.tag(word_tokenize(i))[0][1]:\n",
    "                                flag = False\n",
    "                                sim_vec.append(temp)\n",
    "                            elif k==9:\n",
    "                                    sim_vec.append(' ')\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "\n",
    "            else:\n",
    "                sim_vec.append(' ')\n",
    "    \n",
    "    elif label == '__label__0':\n",
    "        for i in tokens:\n",
    "            flag = True\n",
    "            if i not in stop_words and i in sim_dict_0.keys():\n",
    "                for k in range(7):\n",
    "                    if flag:\n",
    "                        try: \n",
    "                            sim_word = sim_dict_0[i][k]\n",
    "                            temp = sim_word\n",
    "                            \n",
    "                            if rec_diff(i, temp) and tagger.tag(word_tokenize(temp))[0][1] == tagger.tag(word_tokenize(i))[0][1]:\n",
    "                                flag = False\n",
    "                                sim_vec.append(temp)\n",
    "                            elif k==9:\n",
    "                                    sim_vec.append(' ')\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "\n",
    "            else:\n",
    "                sim_vec.append(' ')\n",
    "\n",
    "                \n",
    "    else:\n",
    "        for i in tokens:\n",
    "            flag = True\n",
    "            if i not in stop_words and i in sim_dict_neg.keys():\n",
    "                for k in range(7):\n",
    "                    if flag:\n",
    "                        #try: \n",
    "                            sim_word = sim_dict_neg[i][k]\n",
    "                            temp = sim_word\n",
    "                            if rec_diff(i, temp) and tagger.tag(word_tokenize(temp))[0][1] == tagger.tag(word_tokenize(i))[0][1]:\n",
    "                                flag = False\n",
    "                                sim_vec.append(temp)\n",
    "                            elif k==9:\n",
    "                                    sim_vec.append(' ')\n",
    "                        #except IndexError:\n",
    "                            #pass\n",
    "\n",
    "            else:\n",
    "                sim_vec.append(' ')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return sim_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d82071a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ', ' ', 'گوشت', 'صفها', 'دهیار', 'بن', ' ', 'گوشت', ' ', 'گوشت', ' ', ' ', 'دستمزد', ' ', 'گوشت', 'کارهامو']\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', 'اسفندماه', 'صفها', 'دراثر', 'اسکان', ' ', 'یخ', ' ', 'یخ', ' ', ' ', 'عمری', ' ', 'برآورم', 'بن']\n",
      "__label__neg آقای رشید پور به روستای ما بن گوشت یخ زده دادن به شورا و دهیار  دو نفری همه را کشیدن بالا  از توابع قزوین\n"
     ]
    }
   ],
   "source": [
    "print(change_word_cond(train['sentiment_comment'][4]))\n",
    "print(change_word(train['sentiment_comment'][4]))\n",
    "print(train['sentiment_comment'][4])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "469e4faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 5838/5838 [00:05<00:00, 1101.23it/s]\n"
     ]
    }
   ],
   "source": [
    "augmented_list_cond = []\n",
    "for i in tqdm.tqdm((train['sentiment_comment'])):\n",
    "    comment = i\n",
    "    candidate = change_word_cond(comment)\n",
    "    augmented_list_cond.append(generate_sentence_permutations(candidate, word_tokenize(comment)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95f0c1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7369\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to hold the 1D list\n",
    "one_d_list_cond = []\n",
    "\n",
    "# Use nested for loops to iterate over the sublists and items\n",
    "for sublist in augmented_list_cond:\n",
    "    for item in sublist:\n",
    "        one_d_list_cond.append(item)\n",
    "\n",
    "print(len(one_d_list_cond))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cedacc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__neg از محمد اصفهانی دعوت نکنین</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__neg از شاهکارش خودمونم اموزش خورشیدوا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__1 سلام با تشکر فراوان از برنامه پر ان...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__neg رشیدپور از چشم همه افتاده نون به ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__0 سالار عقیلی شماره ۶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7364</th>\n",
       "      <td>__label__neg مسوولان در حال شمردن پولاشونن وقت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7365</th>\n",
       "      <td>__label__neg چطوری سوپاپ اطمینان کی میخوای خجا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7366</th>\n",
       "      <td>__label__0 به آقا تنابنده بگید یه تیکه شمالی ح...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7367</th>\n",
       "      <td>__label__0 به استخر رئالی بسازند یه تنابنده جد...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>__label__1 اقای رضا رشید پور عزیز</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7369 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sentiment_comment\n",
       "0               __label__neg از محمد اصفهانی دعوت نکنین\n",
       "1     __label__neg از شاهکارش خودمونم اموزش خورشیدوا...\n",
       "2     __label__1 سلام با تشکر فراوان از برنامه پر ان...\n",
       "3     __label__neg رشیدپور از چشم همه افتاده نون به ...\n",
       "4                        __label__0 سالار عقیلی شماره ۶\n",
       "...                                                 ...\n",
       "7364  __label__neg مسوولان در حال شمردن پولاشونن وقت...\n",
       "7365  __label__neg چطوری سوپاپ اطمینان کی میخوای خجا...\n",
       "7366  __label__0 به آقا تنابنده بگید یه تیکه شمالی ح...\n",
       "7367  __label__0 به استخر رئالی بسازند یه تنابنده جد...\n",
       "7368                  __label__1 اقای رضا رشید پور عزیز\n",
       "\n",
       "[7369 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conditional = pd.DataFrame(one_d_list_cond)\n",
    "df_conditional.rename(columns = { 0 :'sentiment_comment', 1: 'numb' }, inplace = True)\n",
    "df_conditional[df_conditional['sentiment_comment'].str.strip().astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a13601ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conditional['sentiment_comment'].to_csv('cond_aug_TFIDF.train', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb7e5022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:   11 Best score:  0.759630 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  14907\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  149268 lr:  0.000000 avg.loss:  0.425270 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model3 = fasttext.train_supervised(\"cond_aug_TFIDF.train\", dim=300, autotuneValidationFile='t.valid', pretrainedVectors='wiki.fa.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb315acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 0.7226074895977809, 0.7226074895977809)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.test('t.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8cfc3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 72.67683772538142%\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(np.array(test).shape[0]):\n",
    "    if model3.predict(np.array(test['comment'])[i])[0][0] == np.array(test['sentiment'])[i] : \n",
    "        acc += 1\n",
    "print(f'accuracy is {acc/np.array(test).shape[0] *100}%')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4028b",
   "metadata": {},
   "source": [
    "### perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "498aca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = [list_[1] for list_ in augmented_list_cond if len(list_) ==2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63b6678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sentences = [' '.join(word_tokenize(sent)[1:]) for sent in result2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d02b1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__1',), array([0.86702371]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.predict(generated_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c38a748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: [1.0122513]\n"
     ]
    }
   ],
   "source": [
    "n = sum(len(sentence.split()) for sentence in generated_sentences)\n",
    "log_probs = [np.log2(model3.predict(sentence, k=1)[1])for sentence in generated_sentences if (model3.predict(sentence, k=1)[1])<1 ]\n",
    "perplexity = 2 ** (-1/n * sum(log_probs))\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742c014",
   "metadata": {},
   "source": [
    "### diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d4aaab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity: 0.7162911295890808\n"
     ]
    }
   ],
   "source": [
    "vectors = [model3.get_sentence_vector(sentence) for sentence in generated_sentences]\n",
    "similarities = cosine_similarity(vectors)\n",
    "diversity = 1 - similarities.mean()\n",
    "print(\"Diversity:\", diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42043fb7",
   "metadata": {},
   "source": [
    "### ارزیابی انسانی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79a81e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org1 : __label__neg از محمد اصفهانی دعوت نکنین \n",
      " gen1 : __label__neg از شاهکارش خودمونم اموزش خورشیدواصلاتموم \n",
      "\n",
      "org2 : __label__0 سالار عقیلی شماره ۶ \n",
      " gen2 : __label__0 آقایان وکیلم حسینی ۶ \n",
      "\n",
      "org3 : __label__1 یزد خودونم برف أومده إقه خش شده \n",
      " gen3 : __label__1 سیدمحمد خورشیذ برف خورشیذ خورشیذ خورشیذ شده \n",
      "\n",
      "org4 : __label__0 مناظره بین دکتر سبطی وآقای بطحایی \n",
      " gen4 : __label__0 آموزش ودکتر طب آقائی بارنگ دکتر \n",
      "\n",
      "org5 : __label__1 همه چیز از اوست \n",
      " gen5 : __label__1 همه خوبید از خوبه \n",
      "\n",
      "org6 : __label__0 مهران مدیری و خود شما آقای رشید پور عزیز \n",
      " gen6 : __label__0 _علی کریمی و شما شما آقای رشید پور عزیز \n",
      "\n",
      "org7 : __label__neg برنامه تون و تمام نکنید لطفا \n",
      " gen7 : __label__neg برنامه تون و تمام وجدانند لطفا \n",
      "\n",
      "org8 : __label__0 قسمت سانسور شده چیا بودن \n",
      " gen8 : __label__0 روایت لطف شده خنج سانسور \n",
      "\n",
      "org9 : __label__0 همه بازماندگانی که سال NUM۲ سلامت به پایان رساندن \n",
      " gen9 : __label__0 همه سال که سال NUM۲ سال به انتظارمان سال \n",
      "\n",
      "org10 : __label__0 محسن از مبا \n",
      " gen10 : __label__0 مبارکی از خمسه \n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "res = [list_ for list_ in augmented_list_cond if len(list_) ==2]\n",
    "for i in range(10):\n",
    "    print(f'org{i+1} : {res[i][0]} \\n gen{i+1} : {res[i][1]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6b9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
